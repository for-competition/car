{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/light/App/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">sentiment_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>价格</th>\n",
       "      <td>1273.0</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>0.450581</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>内饰</th>\n",
       "      <td>536.0</td>\n",
       "      <td>-0.065299</td>\n",
       "      <td>0.700753</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>动力</th>\n",
       "      <td>2732.0</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.528218</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>外观</th>\n",
       "      <td>489.0</td>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.680476</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>安全性</th>\n",
       "      <td>573.0</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>0.580744</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>操控</th>\n",
       "      <td>1036.0</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.620135</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>油耗</th>\n",
       "      <td>1082.0</td>\n",
       "      <td>0.012015</td>\n",
       "      <td>0.516914</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>空间</th>\n",
       "      <td>442.0</td>\n",
       "      <td>0.196833</td>\n",
       "      <td>0.679929</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>舒适性</th>\n",
       "      <td>931.0</td>\n",
       "      <td>-0.155747</td>\n",
       "      <td>0.608556</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>配置</th>\n",
       "      <td>853.0</td>\n",
       "      <td>-0.039859</td>\n",
       "      <td>0.565690</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment_value                                             \n",
       "                  count      mean       std  min  25%  50%  75%  max\n",
       "subject                                                             \n",
       "价格               1273.0 -0.024352  0.450581 -1.0  0.0  0.0  0.0  1.0\n",
       "内饰                536.0 -0.065299  0.700753 -1.0 -1.0  0.0  0.0  1.0\n",
       "动力               2732.0  0.002196  0.528218 -1.0  0.0  0.0  0.0  1.0\n",
       "外观                489.0  0.008180  0.680476 -1.0  0.0  0.0  0.0  1.0\n",
       "安全性               573.0  0.012216  0.580744 -1.0  0.0  0.0  0.0  1.0\n",
       "操控               1036.0  0.175676  0.620135 -1.0  0.0  0.0  1.0  1.0\n",
       "油耗               1082.0  0.012015  0.516914 -1.0  0.0  0.0  0.0  1.0\n",
       "空间                442.0  0.196833  0.679929 -1.0  0.0  0.0  1.0  1.0\n",
       "舒适性               931.0 -0.155747  0.608556 -1.0 -1.0  0.0  0.0  1.0\n",
       "配置                853.0 -0.039859  0.565690 -1.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('subject').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>sentiment_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vUXizsqexyZVRdFH</td>\n",
       "      <td>因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。</td>\n",
       "      <td>价格</td>\n",
       "      <td>0</td>\n",
       "      <td>影响</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4QroPd9hNfnCHVt7</td>\n",
       "      <td>四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。</td>\n",
       "      <td>价格</td>\n",
       "      <td>-1</td>\n",
       "      <td>高</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content_id                                        content subject  \\\n",
       "0  vUXizsqexyZVRdFH       因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。      价格   \n",
       "1  4QroPd9hNfnCHVt7  四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。      价格   \n",
       "\n",
       "   sentiment_value sentiment_word  \n",
       "0                0             影响  \n",
       "1               -1              高  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_df = pd.get_dummies(df, columns=['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>sentiment_word</th>\n",
       "      <th>subject_价格</th>\n",
       "      <th>subject_内饰</th>\n",
       "      <th>subject_动力</th>\n",
       "      <th>subject_外观</th>\n",
       "      <th>subject_安全性</th>\n",
       "      <th>subject_操控</th>\n",
       "      <th>subject_油耗</th>\n",
       "      <th>subject_空间</th>\n",
       "      <th>subject_舒适性</th>\n",
       "      <th>subject_配置</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vUXizsqexyZVRdFH</td>\n",
       "      <td>因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。</td>\n",
       "      <td>0</td>\n",
       "      <td>影响</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4QroPd9hNfnCHVt7</td>\n",
       "      <td>四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。</td>\n",
       "      <td>-1</td>\n",
       "      <td>高</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content_id                                        content  \\\n",
       "0  vUXizsqexyZVRdFH       因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。   \n",
       "1  4QroPd9hNfnCHVt7  四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。   \n",
       "\n",
       "   sentiment_value sentiment_word  subject_价格  subject_内饰  subject_动力  \\\n",
       "0                0             影响           1           0           0   \n",
       "1               -1              高           1           0           0   \n",
       "\n",
       "   subject_外观  subject_安全性  subject_操控  subject_油耗  subject_空间  subject_舒适性  \\\n",
       "0           0            0           0           0           0            0   \n",
       "1           0            0           0           0           0            0   \n",
       "\n",
       "   subject_配置  \n",
       "0           0  \n",
       "1           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = '价格'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        stops = set([v.strip() for v in f.readlines()])\n",
    "    return stops\n",
    "stops = load_stopwords('../data/stopwords.txt')\n",
    "def seg(text):\n",
    "    result = [word for word in jieba.cut(text) if word not in stops]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, serial):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in serial:\n",
    "        for word in seg(sentence):\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.815 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 16280\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, oh_df['content'])\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_file, header=True):\n",
    "    # Load embeddings\n",
    "    embeddings_index = {}\n",
    "    with open(embedding_file) as f:\n",
    "        if header:\n",
    "            print(f.readline())\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                break\n",
    "            try:\n",
    "                word, vec = line.split(' ', 1)\n",
    "            except:\n",
    "                print(line)\n",
    "            vec = [float(v) for v in vec.split(' ')]\n",
    "            embedding = np.asarray(vec, dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(word_embedding_index, path):\n",
    "    first = True\n",
    "    with open(path, 'w') as f:\n",
    "        for k, rep in word_embedding_index.items():\n",
    "            line = '{} {}'.format(k, ' '.join([str(v) for v in rep]))\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "            f.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8e1080c14f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/cliped_vec.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-19a08a0875b3>\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(embedding_file, header)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings('../data/cliped_vec.vec', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2int2vocab(word_counts, embeddings_index):\n",
    "    # Limit the vocab that we will use to words that appear ≥ threshold or are in embedding\n",
    "\n",
    "    #dictionary to convert words to integers\n",
    "    vocab_to_int = {} \n",
    "\n",
    "    value = 0\n",
    "    for word, count in word_counts.items():\n",
    "        if word in embeddings_index:\n",
    "            vocab_to_int[word] = value\n",
    "            value += 1\n",
    "\n",
    "    # Special tokens that will be added to our vocab\n",
    "    codes = [\"<UNK>\",\"<PAD>\"]   \n",
    "\n",
    "    # Add codes to vocab\n",
    "    for code in codes:\n",
    "        vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "    # Dictionary to convert integers to words\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "\n",
    "    usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "    print(\"Total number of unique words:\", len(word_counts))\n",
    "    print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "    print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2int, int2vocab = vocab2int2vocab(word_counts, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocab_to_int, embeddings_index):\n",
    "    # Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "    embedding_dim = 300\n",
    "    nb_words = len(vocab_to_int)\n",
    "    not_in = 0\n",
    "    # Create matrix with default values of zero\n",
    "    word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    for word, i in vocab_to_int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_embedding_matrix[i] = embeddings_index[word]\n",
    "        elif word == '<PAD>':\n",
    "            not_in += 1\n",
    "            new_embedding = np.zeros((embedding_dim, ))\n",
    "            embeddings_index[word] = new_embedding\n",
    "            word_embedding_matrix[i] = new_embedding\n",
    "        else:\n",
    "            not_in += 1\n",
    "            # If word not in CN, create a random embedding for it\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            embeddings_index[word] = new_embedding\n",
    "            word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "    # Check if value matches len(vocab_to_int)\n",
    "    print(len(word_embedding_matrix))\n",
    "    print('not in:', not_in)\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix(vocab2int, embeddings_index)\n",
    "save_embedding(embeddings_index, '../data/filled.es.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(serial, vocab_to_int, word_count, unk_count):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in serial:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df, theme):\n",
    "    theme_col = 'subject_{}'.format(theme)\n",
    "    for row in df.iterrows():\n",
    "        value = [0, 0, 0, 0]\n",
    "        if row[theme_col] == 0:\n",
    "            key = 0\n",
    "        else:\n",
    "            key = int(row['sentiment_value'] + 2)\n",
    "        value[key] = 1\n",
    "        yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_es, word_count, unk_count = convert_to_ints(oh_df['content'], vocab2int, word_count, unk_count)\n",
    "label = list(oh_df, theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int, max_sentence=40):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    return [sentence[:max_sentence] + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence[:max_sentence])) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(X, Y, val_size=3000):\n",
    "    indices = np.arange(len(X1))\n",
    "    np.random.shuffle(indices)\n",
    "    val_idxs = set(indices[:val_size])\n",
    "    val_X, val_Y = [], []\n",
    "    train_X, train_Y = [], []\n",
    "    match_count = 0\n",
    "    for idx in indices:\n",
    "        if idx in val_idxs:\n",
    "            val_X1.append(X[idx])\n",
    "            val_Y.append(Y[idx])\n",
    "            match_count += Y[idx]\n",
    "        else:\n",
    "            train_X.append(X[idx])\n",
    "            train_Y.append(Y[idx])\n",
    "    print('match rate', match_count/val_size)\n",
    "    return (train_X, train_Y), (val_X, val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y=None, num_epochs=4, batch_size=10, echo=True, shuffle=True):\n",
    "    \n",
    "    batch_x = []\n",
    "    if Y:\n",
    "        batch_y = []\n",
    "    indices = np.arange(len(X))\n",
    "    for epoch in range(num_epochs):\n",
    "        if echo:\n",
    "            print('-----------epoch:', epoch)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for idx in indices:\n",
    "            x = X[idx]\n",
    "            if Y:\n",
    "                y = Y[idx]\n",
    "            batch_x.append(x)\n",
    "            if Y:\n",
    "                batch_y.append(y)\n",
    "            if len(batch_x1) == batch_size:\n",
    "                bx = pad_sentence_batch(batch_x, es_vocab2int)\n",
    "                if Y:\n",
    "                    yield bx, batch_y\n",
    "                else:\n",
    "                    yield bx\n",
    "                batch_x = []\n",
    "                if Y:\n",
    "                    batch_y = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textcnn import TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_Y), (val_X, val_Y) = split_train_val(int_es, label, val_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = './textcnn.ckpt'\n",
    "work_dir = 'my_graph'\n",
    "test_size = 1000\n",
    "test_loss = 10000\n",
    "\n",
    "stop_early = 0\n",
    "stop = 40\n",
    "\n",
    "print_step = 3\n",
    "update_step = 10\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    cnn = TextCNN(seq_len=80, embedding=es_embedding_matrix, filter_sizes=[3, 4, 5], num_filters=256, num_classes=4, hidden_units=128, l2_reg=0)\n",
    "    cnn.create_placeholder()\n",
    "    cnn.create_variable()\n",
    "    cnn.create_model()\n",
    "    cnn.create_loss(lr=0.0001)\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        writer = tf.summary.FileWriter(work_dir, sess.graph)\n",
    "        \n",
    "        # global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        for batch_x, batch_y in get_batch(train_X, train_Y, num_epochs=8, batch_size=30):\n",
    "            feed_dict = {\n",
    "                cnn.x: batch_x,\n",
    "                cnn.y: batch_y,\n",
    "                cnn.keep_prob: 0.5,\n",
    "            }\n",
    "            # loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            # loss_smry, loss, _ = sess.run([loss_summary, cnn.loss, cnn.train_op], feed_dict=feed_dict)\n",
    "            loss, _ = sess.run([cnn.loss, cnn.train_op], feed_dict=feed_dict)\n",
    "            step += 1\n",
    "            \n",
    "            # writer.add_summary(loss_smry, step)\n",
    "           \n",
    "            \n",
    "            if step % print_step == 0:\n",
    "                print('step:', step, ', loss:', loss)\n",
    "            if step % update_step == 0:\n",
    "                val_loss = []\n",
    "                for batch_x, batch_y in get_batch(val_X, val_Y, num_epochs=1, batch_size=20, echo=False):\n",
    "                    feed_dict = {\n",
    "                        cnn.x: batch_x,\n",
    "                        cnn.y: batch_y,\n",
    "                        cnn.keep_prob: 1,\n",
    "                    }\n",
    "                    loss, _ = sess.run([cnn.loss, cnn.train_op], feed_dict=feed_dict)\n",
    "                    val_loss.append(loss)\n",
    "                update_loss = sum(val_loss)/len(val_loss)\n",
    "                print('test loss:', update_loss)\n",
    "                if update_loss <= test_loss:\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "                    test_loss = update_loss\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
