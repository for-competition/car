{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/light/App/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">sentiment_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>价格</th>\n",
       "      <td>1273.0</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>0.450581</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>内饰</th>\n",
       "      <td>536.0</td>\n",
       "      <td>-0.065299</td>\n",
       "      <td>0.700753</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>动力</th>\n",
       "      <td>2732.0</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.528218</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>外观</th>\n",
       "      <td>489.0</td>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.680476</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>安全性</th>\n",
       "      <td>573.0</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>0.580744</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>操控</th>\n",
       "      <td>1036.0</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.620135</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>油耗</th>\n",
       "      <td>1082.0</td>\n",
       "      <td>0.012015</td>\n",
       "      <td>0.516914</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>空间</th>\n",
       "      <td>442.0</td>\n",
       "      <td>0.196833</td>\n",
       "      <td>0.679929</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>舒适性</th>\n",
       "      <td>931.0</td>\n",
       "      <td>-0.155747</td>\n",
       "      <td>0.608556</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>配置</th>\n",
       "      <td>853.0</td>\n",
       "      <td>-0.039859</td>\n",
       "      <td>0.565690</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment_value                                             \n",
       "                  count      mean       std  min  25%  50%  75%  max\n",
       "subject                                                             \n",
       "价格               1273.0 -0.024352  0.450581 -1.0  0.0  0.0  0.0  1.0\n",
       "内饰                536.0 -0.065299  0.700753 -1.0 -1.0  0.0  0.0  1.0\n",
       "动力               2732.0  0.002196  0.528218 -1.0  0.0  0.0  0.0  1.0\n",
       "外观                489.0  0.008180  0.680476 -1.0  0.0  0.0  0.0  1.0\n",
       "安全性               573.0  0.012216  0.580744 -1.0  0.0  0.0  0.0  1.0\n",
       "操控               1036.0  0.175676  0.620135 -1.0  0.0  0.0  1.0  1.0\n",
       "油耗               1082.0  0.012015  0.516914 -1.0  0.0  0.0  0.0  1.0\n",
       "空间                442.0  0.196833  0.679929 -1.0  0.0  0.0  1.0  1.0\n",
       "舒适性               931.0 -0.155747  0.608556 -1.0 -1.0  0.0  0.0  1.0\n",
       "配置                853.0 -0.039859  0.565690 -1.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('subject').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>sentiment_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vUXizsqexyZVRdFH</td>\n",
       "      <td>因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。</td>\n",
       "      <td>价格</td>\n",
       "      <td>0</td>\n",
       "      <td>影响</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4QroPd9hNfnCHVt7</td>\n",
       "      <td>四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。</td>\n",
       "      <td>价格</td>\n",
       "      <td>-1</td>\n",
       "      <td>高</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content_id                                        content subject  \\\n",
       "0  vUXizsqexyZVRdFH       因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。      价格   \n",
       "1  4QroPd9hNfnCHVt7  四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。      价格   \n",
       "\n",
       "   sentiment_value sentiment_word  \n",
       "0                0             影响  \n",
       "1               -1              高  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_df = pd.get_dummies(df, columns=['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>sentiment_word</th>\n",
       "      <th>subject_价格</th>\n",
       "      <th>subject_内饰</th>\n",
       "      <th>subject_动力</th>\n",
       "      <th>subject_外观</th>\n",
       "      <th>subject_安全性</th>\n",
       "      <th>subject_操控</th>\n",
       "      <th>subject_油耗</th>\n",
       "      <th>subject_空间</th>\n",
       "      <th>subject_舒适性</th>\n",
       "      <th>subject_配置</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vUXizsqexyZVRdFH</td>\n",
       "      <td>因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。</td>\n",
       "      <td>0</td>\n",
       "      <td>影响</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4QroPd9hNfnCHVt7</td>\n",
       "      <td>四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。</td>\n",
       "      <td>-1</td>\n",
       "      <td>高</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content_id                                        content  \\\n",
       "0  vUXizsqexyZVRdFH       因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。   \n",
       "1  4QroPd9hNfnCHVt7  四驱价格貌似挺高的，高的可以看齐XC60了，看实车前脸有点违和感。不过大众的车应该不会差。   \n",
       "\n",
       "   sentiment_value sentiment_word  subject_价格  subject_内饰  subject_动力  \\\n",
       "0                0             影响           1           0           0   \n",
       "1               -1              高           1           0           0   \n",
       "\n",
       "   subject_外观  subject_安全性  subject_操控  subject_油耗  subject_空间  subject_舒适性  \\\n",
       "0           0            0           0           0           0            0   \n",
       "1           0            0           0           0           0            0   \n",
       "\n",
       "   subject_配置  \n",
       "0           0  \n",
       "1           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = '价格'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        stops = set([v.strip() for v in f.readlines()])\n",
    "    return stops\n",
    "stops = load_stopwords('../data/stopwords.txt')\n",
    "def seg(text):\n",
    "    result = [word for word in jieba.cut(text) if word not in stops]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, serial):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in serial:\n",
    "        for word in seg(sentence):\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.819 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 16280\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, oh_df['content'])\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_file, header=True):\n",
    "    # Load embeddings\n",
    "    embeddings_index = {}\n",
    "    with open(embedding_file) as f:\n",
    "        if header:\n",
    "            print(f.readline())\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                break\n",
    "            try:\n",
    "                word, vec = line.split(' ', 1)\n",
    "            except:\n",
    "                print(line)\n",
    "            vec = [float(v) for v in vec.split(' ')]\n",
    "            embedding = np.asarray(vec, dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(word_embedding_index, path):\n",
    "    first = True\n",
    "    with open(path, 'w') as f:\n",
    "        for k, rep in word_embedding_index.items():\n",
    "            line = '{} {}'.format(k, ' '.join([str(v) for v in rep]))\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "            f.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 12666\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings('../data/cliped_vec.vec', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2int2vocab(word_counts, embeddings_index):\n",
    "    # Limit the vocab that we will use to words that appear ≥ threshold or are in embedding\n",
    "\n",
    "    #dictionary to convert words to integers\n",
    "    vocab_to_int = {} \n",
    "\n",
    "    value = 0\n",
    "    for word, count in word_counts.items():\n",
    "        if word in embeddings_index:\n",
    "            vocab_to_int[word] = value\n",
    "            value += 1\n",
    "\n",
    "    # Special tokens that will be added to our vocab\n",
    "    codes = [\"<UNK>\",\"<PAD>\"]   \n",
    "\n",
    "    # Add codes to vocab\n",
    "    for code in codes:\n",
    "        vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "    # Dictionary to convert integers to words\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "\n",
    "    usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "    print(\"Total number of unique words:\", len(word_counts))\n",
    "    print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "    print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 16280\n",
      "Number of words we will use: 11676\n",
      "Percent of words we will use: 71.72%\n"
     ]
    }
   ],
   "source": [
    "vocab2int, int2vocab = vocab2int2vocab(word_counts, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocab_to_int, embeddings_index):\n",
    "    # Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "    embedding_dim = 300\n",
    "    nb_words = len(vocab_to_int)\n",
    "    not_in = 0\n",
    "    # Create matrix with default values of zero\n",
    "    word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    for word, i in vocab_to_int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_embedding_matrix[i] = embeddings_index[word]\n",
    "        elif word == '<PAD>':\n",
    "            not_in += 1\n",
    "            new_embedding = np.zeros((embedding_dim, ))\n",
    "            embeddings_index[word] = new_embedding\n",
    "            word_embedding_matrix[i] = new_embedding\n",
    "        else:\n",
    "            not_in += 1\n",
    "            # If word not in CN, create a random embedding for it\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            embeddings_index[word] = new_embedding\n",
    "            word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "    # Check if value matches len(vocab_to_int)\n",
    "    print(len(word_embedding_matrix))\n",
    "    print('not in:', not_in)\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11676\n",
      "not in: 2\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_matrix(vocab2int, embeddings_index)\n",
    "save_embedding(embeddings_index, '../data/filled.es.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(serial, vocab_to_int, word_count, unk_count):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in serial:\n",
    "        sentence_ints = []\n",
    "        for word in seg(sentence):\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df, theme):\n",
    "    theme_col = 'subject_{}'.format(theme)\n",
    "    for index, row in df.iterrows():\n",
    "        # value = [0, 0, 0, 0]\n",
    "        # print(row)\n",
    "        if row[theme_col] == 0:\n",
    "            key = 0\n",
    "        else:\n",
    "            key = int(row['sentiment_value'] + 2)\n",
    "        yield key\n",
    "        # value[key] = 1\n",
    "        # yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_es, word_count, unk_count = convert_to_ints(oh_df['content'], vocab2int, word_count, unk_count)\n",
    "label = list(get_labels(oh_df, theme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int, max_sentence=80):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    return [sentence[:max_sentence] + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence[:max_sentence])) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(X, Y, val_size=3000):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    val_idxs = set(indices[:val_size])\n",
    "    val_X, val_Y = [], []\n",
    "    train_X, train_Y = [], []\n",
    "    match_count = 0\n",
    "    for idx in indices:\n",
    "        if idx in val_idxs:\n",
    "            val_X.append(X[idx])\n",
    "            val_Y.append(Y[idx])\n",
    "        else:\n",
    "            train_X.append(X[idx])\n",
    "            train_Y.append(Y[idx])\n",
    "    return (train_X, train_Y), (val_X, val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y=None, num_epochs=4, batch_size=10, echo=True, shuffle=True):\n",
    "    \n",
    "    batch_x = []\n",
    "    if Y:\n",
    "        batch_y = []\n",
    "    indices = np.arange(len(X))\n",
    "    for epoch in range(num_epochs):\n",
    "        if echo:\n",
    "            print('-----------epoch:', epoch)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for idx in indices:\n",
    "            x = X[idx]\n",
    "            if Y:\n",
    "                y = Y[idx]\n",
    "            batch_x.append(x)\n",
    "            if Y:\n",
    "                batch_y.append(y)\n",
    "            if len(batch_x) == batch_size:\n",
    "                bx = pad_sentence_batch(batch_x, vocab2int)\n",
    "                if Y:\n",
    "                    yield bx, batch_y\n",
    "                else:\n",
    "                    yield bx\n",
    "                batch_x = []\n",
    "                if Y:\n",
    "                    batch_y = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textcnn import TextCNN\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_Y), (val_X, val_Y) = split_train_val(int_es, label, val_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9947"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 80), dtype=int32) Tensor(\"y:0\", shape=(?,), dtype=int32) Tensor(\"keep_prob:0\", dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 1, 1, 768), dtype=float32)\n",
      "Tensor(\"dropout/dropout/mul:0\", shape=(?, 768), dtype=float32)\n",
      "WARNING:tensorflow:From /home/light/App/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From /home/light/Coding/car/code/textcnn.py:92: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "-----------epoch: 0\n",
      "step: 5 , loss: 1.6202083\n",
      "step: 10 , loss: 1.6048535\n",
      "step: 15 , loss: 1.626726\n",
      "step: 20 , loss: 1.6361796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/light/App/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.5847752332687377 , f1: 0.045454545454545456\n",
      "New Record!\n",
      "step: 25 , loss: 1.6835632\n",
      "step: 30 , loss: 1.6409535\n",
      "step: 35 , loss: 1.6053871\n",
      "step: 40 , loss: 1.5560058\n",
      "test loss: 1.5623201131820679 , f1: 0.125\n",
      "New Record!\n",
      "step: 45 , loss: 1.4760184\n",
      "step: 50 , loss: 1.6024399\n",
      "step: 55 , loss: 1.5318824\n",
      "step: 60 , loss: 1.6122329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/light/App/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.5407095511754354 , f1: 0.08125\n",
      "No Improvement.\n",
      "step: 65 , loss: 1.5756677\n",
      "step: 70 , loss: 1.519667\n",
      "step: 75 , loss: 1.5455898\n",
      "step: 80 , loss: 1.5852976\n",
      "test loss: 1.5193959871927898 , f1: 0.0\n",
      "No Improvement.\n",
      "step: 85 , loss: 1.5692003\n",
      "step: 90 , loss: 1.4900525\n",
      "step: 95 , loss: 1.543061\n",
      "step: 100 , loss: 1.4951389\n",
      "test loss: 1.498287590344747 , f1: 0.04761904761904762\n",
      "No Improvement.\n",
      "step: 105 , loss: 1.4638535\n",
      "step: 110 , loss: 1.4752098\n",
      "step: 115 , loss: 1.460563\n",
      "step: 120 , loss: 1.5317277\n",
      "test loss: 1.4779767910639445 , f1: 0.024999999999999998\n",
      "No Improvement.\n",
      "step: 125 , loss: 1.5072783\n",
      "step: 130 , loss: 1.525418\n",
      "step: 135 , loss: 1.4753922\n",
      "step: 140 , loss: 1.4518906\n",
      "test loss: 1.4580783605575562 , f1: 0.07894736842105263\n",
      "No Improvement.\n",
      "step: 145 , loss: 1.456187\n",
      "step: 150 , loss: 1.431739\n",
      "step: 155 , loss: 1.5157124\n",
      "step: 160 , loss: 1.5400963\n",
      "test loss: 1.4378921031951903 , f1: 0.10714285714285715\n",
      "No Improvement.\n",
      "step: 165 , loss: 1.4614568\n",
      "step: 170 , loss: 1.4254769\n",
      "step: 175 , loss: 1.3896596\n",
      "step: 180 , loss: 1.4069505\n",
      "test loss: 1.4185863653818767 , f1: 0.05263157894736842\n",
      "No Improvement.\n",
      "step: 185 , loss: 1.4501506\n",
      "step: 190 , loss: 1.4155114\n",
      "step: 195 , loss: 1.4688351\n",
      "step: 200 , loss: 1.5305516\n",
      "test loss: 1.3995206356048584 , f1: 0.16666666666666669\n",
      "New Record!\n",
      "step: 205 , loss: 1.3933657\n",
      "step: 210 , loss: 1.3635336\n",
      "step: 215 , loss: 1.3944058\n",
      "step: 220 , loss: 1.381321\n",
      "test loss: 1.3815640608469646 , f1: 0.29629629629629634\n",
      "New Record!\n",
      "step: 225 , loss: 1.390788\n",
      "step: 230 , loss: 1.4182358\n",
      "step: 235 , loss: 1.3673867\n",
      "step: 240 , loss: 1.401438\n",
      "test loss: 1.3632262786229452 , f1: 0.0909090909090909\n",
      "No Improvement.\n",
      "step: 245 , loss: 1.3989487\n",
      "step: 250 , loss: 1.4104518\n",
      "step: 255 , loss: 1.3436753\n",
      "step: 260 , loss: 1.4229703\n",
      "test loss: 1.3456610361735026 , f1: 0.46078431372549017\n",
      "New Record!\n",
      "step: 265 , loss: 1.4090914\n",
      "step: 270 , loss: 1.3460946\n",
      "step: 275 , loss: 1.363325\n",
      "step: 280 , loss: 1.3982164\n",
      "test loss: 1.3279974142710367 , f1: 0.2708333333333333\n",
      "No Improvement.\n",
      "step: 285 , loss: 1.3234303\n",
      "step: 290 , loss: 1.3304104\n",
      "step: 295 , loss: 1.3376561\n",
      "step: 300 , loss: 1.345945\n",
      "test loss: 1.3107017755508423 , f1: 0.2058823529411765\n",
      "No Improvement.\n",
      "step: 305 , loss: 1.2780368\n",
      "step: 310 , loss: 1.3157645\n",
      "step: 315 , loss: 1.3288652\n",
      "step: 320 , loss: 1.373734\n",
      "test loss: 1.293695871035258 , f1: 0.4444444444444445\n",
      "No Improvement.\n",
      "-----------epoch: 1\n",
      "step: 325 , loss: 1.376914\n",
      "step: 330 , loss: 1.3019749\n",
      "step: 335 , loss: 1.342582\n",
      "step: 340 , loss: 1.3323611\n",
      "test loss: 1.2772233406702678 , f1: 0.2361111111111111\n",
      "No Improvement.\n",
      "step: 345 , loss: 1.251771\n",
      "step: 350 , loss: 1.287827\n",
      "step: 355 , loss: 1.2550967\n",
      "step: 360 , loss: 1.2056913\n",
      "test loss: 1.2613882780075074 , f1: 0.4736842105263158\n",
      "New Record!\n",
      "step: 365 , loss: 1.1827406\n",
      "step: 370 , loss: 1.2338321\n",
      "step: 375 , loss: 1.2776823\n",
      "step: 380 , loss: 1.2864801\n",
      "test loss: 1.2460099856058757 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 385 , loss: 1.2490711\n",
      "step: 390 , loss: 1.2329812\n",
      "step: 395 , loss: 1.2105049\n",
      "step: 400 , loss: 1.2345511\n",
      "test loss: 1.230609917640686 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 405 , loss: 1.2780395\n",
      "step: 410 , loss: 1.3705935\n",
      "step: 415 , loss: 1.2485673\n",
      "step: 420 , loss: 1.2078264\n",
      "test loss: 1.215455134709676 , f1: 0.48717948717948717\n",
      "New Record!\n",
      "step: 425 , loss: 1.195007\n",
      "step: 430 , loss: 1.2776592\n",
      "step: 435 , loss: 1.2310299\n",
      "step: 440 , loss: 1.2207699\n",
      "test loss: 1.201132837931315 , f1: 0.42857142857142855\n",
      "No Improvement.\n",
      "step: 445 , loss: 1.2213595\n",
      "step: 450 , loss: 1.1652398\n",
      "step: 455 , loss: 1.2831633\n",
      "step: 460 , loss: 1.2506044\n",
      "test loss: 1.1868814706802369 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 465 , loss: 1.199982\n",
      "step: 470 , loss: 1.1630589\n",
      "step: 475 , loss: 1.1810933\n",
      "step: 480 , loss: 1.1945941\n",
      "test loss: 1.1726633787155152 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 485 , loss: 1.1489332\n",
      "step: 490 , loss: 1.1409804\n",
      "step: 495 , loss: 1.1341478\n",
      "step: 500 , loss: 1.1484164\n",
      "test loss: 1.1589762369791667 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 505 , loss: 1.190207\n",
      "step: 510 , loss: 1.1415417\n",
      "step: 515 , loss: 1.1143281\n",
      "step: 520 , loss: 1.13905\n",
      "test loss: 1.145615824063619 , f1: 0.3157894736842105\n",
      "No Improvement.\n",
      "step: 525 , loss: 1.1300304\n",
      "step: 530 , loss: 1.165355\n",
      "step: 535 , loss: 1.1510543\n",
      "step: 540 , loss: 1.1843393\n",
      "test loss: 1.1324174563090006 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 545 , loss: 1.2145991\n",
      "step: 550 , loss: 1.1088741\n",
      "step: 555 , loss: 1.1306653\n",
      "step: 560 , loss: 1.1579486\n",
      "test loss: 1.1198601643244426 , f1: 0.22972972972972971\n",
      "No Improvement.\n",
      "step: 565 , loss: 1.1538904\n",
      "step: 570 , loss: 1.1652994\n",
      "step: 575 , loss: 1.0769072\n",
      "step: 580 , loss: 1.0177308\n",
      "test loss: 1.1074373404184976 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 585 , loss: 1.0849622\n",
      "step: 590 , loss: 1.1408464\n",
      "step: 595 , loss: 1.0453898\n",
      "step: 600 , loss: 1.1820962\n",
      "test loss: 1.0954865535100302 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 605 , loss: 1.1550255\n",
      "step: 610 , loss: 1.0633934\n",
      "step: 615 , loss: 1.027992\n",
      "step: 620 , loss: 1.0433862\n",
      "test loss: 1.0837353706359862 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 625 , loss: 1.0901264\n",
      "step: 630 , loss: 1.1159143\n",
      "step: 635 , loss: 1.1067436\n",
      "step: 640 , loss: 1.0849305\n",
      "test loss: 1.0722909172375996 , f1: 0.22972972972972971\n",
      "No Improvement.\n",
      "-----------epoch: 2\n",
      "step: 645 , loss: 1.0430225\n",
      "step: 650 , loss: 1.0964386\n",
      "step: 655 , loss: 1.0835376\n",
      "step: 660 , loss: 1.063745\n",
      "test loss: 1.06121430794398 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 665 , loss: 0.96614933\n",
      "step: 670 , loss: 1.0109524\n",
      "step: 675 , loss: 1.1146815\n",
      "step: 680 , loss: 1.0673279\n",
      "test loss: 1.0503828605016072 , f1: 0.4117647058823529\n",
      "No Improvement.\n",
      "step: 685 , loss: 1.109996\n",
      "step: 690 , loss: 1.0944816\n",
      "step: 695 , loss: 1.0375347\n",
      "step: 700 , loss: 0.99856895\n",
      "test loss: 1.0396894256273905 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 705 , loss: 0.89004236\n",
      "step: 710 , loss: 1.0117697\n",
      "step: 715 , loss: 0.94639033\n",
      "step: 720 , loss: 1.0644944\n",
      "test loss: 1.0291918873786927 , f1: 1.0\n",
      "New Record!\n",
      "step: 725 , loss: 0.986783\n",
      "step: 730 , loss: 1.1668208\n",
      "step: 735 , loss: 1.0933201\n",
      "step: 740 , loss: 1.084785\n",
      "test loss: 1.0194472710291544 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 745 , loss: 0.964173\n",
      "step: 750 , loss: 0.998477\n",
      "step: 755 , loss: 0.902001\n",
      "step: 760 , loss: 0.9384707\n",
      "test loss: 1.0094316999117534 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 765 , loss: 0.9422796\n",
      "step: 770 , loss: 1.0077982\n",
      "step: 775 , loss: 1.0505118\n",
      "step: 780 , loss: 0.98003125\n",
      "test loss: 0.9998332182566325 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 785 , loss: 0.92830217\n",
      "step: 790 , loss: 0.8388373\n",
      "step: 795 , loss: 1.0495472\n",
      "step: 800 , loss: 1.0425467\n",
      "test loss: 0.990385099252065 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 805 , loss: 0.96247596\n",
      "step: 810 , loss: 0.96734565\n",
      "step: 815 , loss: 1.030339\n",
      "step: 820 , loss: 0.99665034\n",
      "test loss: 0.9813699801762898 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 825 , loss: 1.0134877\n",
      "step: 830 , loss: 0.9298434\n",
      "step: 835 , loss: 0.96585876\n",
      "step: 840 , loss: 0.9679802\n",
      "test loss: 0.9722374399503072 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 845 , loss: 0.9564319\n",
      "step: 850 , loss: 1.0068144\n",
      "step: 855 , loss: 1.0031886\n",
      "step: 860 , loss: 1.036028\n",
      "test loss: 0.9636255343755086 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 865 , loss: 0.8884545\n",
      "step: 870 , loss: 0.9033502\n",
      "step: 875 , loss: 0.92716545\n",
      "step: 880 , loss: 0.9587908\n",
      "test loss: 0.955107553799947 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 885 , loss: 1.0148538\n",
      "step: 890 , loss: 0.8674302\n",
      "step: 895 , loss: 0.96036446\n",
      "step: 900 , loss: 0.9768481\n",
      "test loss: 0.9466981609662374 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 905 , loss: 0.8902647\n",
      "step: 910 , loss: 0.9592962\n",
      "step: 915 , loss: 0.9267264\n",
      "step: 920 , loss: 0.95349574\n",
      "test loss: 0.9386290033658345 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 925 , loss: 0.8716872\n",
      "step: 930 , loss: 0.9026206\n",
      "step: 935 , loss: 0.9118718\n",
      "step: 940 , loss: 0.91927624\n",
      "test loss: 0.9306103746096294 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 945 , loss: 0.94960713\n",
      "step: 950 , loss: 0.8962586\n",
      "step: 955 , loss: 1.041452\n",
      "step: 960 , loss: 0.8381971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9229409257570903 , f1: 0.22222222222222224\n",
      "No Improvement.\n",
      "-----------epoch: 3\n",
      "step: 965 , loss: 0.8675812\n",
      "step: 970 , loss: 0.97074866\n",
      "step: 975 , loss: 0.85505617\n",
      "step: 980 , loss: 0.83300865\n",
      "test loss: 0.9154188156127929 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 985 , loss: 0.8676422\n",
      "step: 990 , loss: 0.8576185\n",
      "step: 995 , loss: 0.9666559\n",
      "step: 1000 , loss: 0.9759339\n",
      "test loss: 0.9079889933268229 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1005 , loss: 1.0154312\n",
      "step: 1010 , loss: 0.8812673\n",
      "step: 1015 , loss: 0.9460286\n",
      "step: 1020 , loss: 0.80418277\n",
      "test loss: 0.9008304953575135 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1025 , loss: 0.8684019\n",
      "step: 1030 , loss: 0.93431205\n",
      "step: 1035 , loss: 0.91892844\n",
      "step: 1040 , loss: 0.85865766\n",
      "test loss: 0.8939104398091634 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 1045 , loss: 0.88534063\n",
      "step: 1050 , loss: 0.77127135\n",
      "step: 1055 , loss: 0.82411844\n",
      "step: 1060 , loss: 0.88456786\n",
      "test loss: 0.8871761480967204 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1065 , loss: 0.93140876\n",
      "step: 1070 , loss: 0.91015905\n",
      "step: 1075 , loss: 0.7957634\n",
      "step: 1080 , loss: 0.86410385\n",
      "test loss: 0.8805072665214538 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 1085 , loss: 0.81318605\n",
      "step: 1090 , loss: 0.715372\n",
      "step: 1095 , loss: 0.9082398\n",
      "step: 1100 , loss: 1.0256454\n",
      "test loss: 0.8743431011835734 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1105 , loss: 0.8713118\n",
      "step: 1110 , loss: 0.8913702\n",
      "step: 1115 , loss: 0.8779511\n",
      "step: 1120 , loss: 0.84920496\n",
      "test loss: 0.8681371053059895 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 1125 , loss: 0.833719\n",
      "step: 1130 , loss: 0.781992\n",
      "step: 1135 , loss: 0.83646464\n",
      "step: 1140 , loss: 0.9266358\n",
      "test loss: 0.8619337916374207 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 1145 , loss: 0.971018\n",
      "step: 1150 , loss: 0.9526597\n",
      "step: 1155 , loss: 0.9317072\n",
      "step: 1160 , loss: 0.88343656\n",
      "test loss: 0.8559338728586833 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1165 , loss: 0.8784151\n",
      "step: 1170 , loss: 0.72388345\n",
      "step: 1175 , loss: 0.775355\n",
      "step: 1180 , loss: 0.8077844\n",
      "test loss: 0.8498930295308431 , f1: 0.21428571428571427\n",
      "No Improvement.\n",
      "step: 1185 , loss: 0.9111216\n",
      "step: 1190 , loss: 0.8772368\n",
      "step: 1195 , loss: 0.8603885\n",
      "step: 1200 , loss: 0.8263492\n",
      "test loss: 0.8444108327229818 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1205 , loss: 0.7651606\n",
      "step: 1210 , loss: 0.7458834\n",
      "step: 1215 , loss: 0.8173647\n",
      "step: 1220 , loss: 0.89476526\n",
      "test loss: 0.8389297842979431 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 1225 , loss: 0.8882454\n",
      "step: 1230 , loss: 0.88476044\n",
      "step: 1235 , loss: 0.71843123\n",
      "step: 1240 , loss: 0.64799815\n",
      "test loss: 0.8335389733314514 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 1245 , loss: 0.7768042\n",
      "step: 1250 , loss: 0.76289004\n",
      "step: 1255 , loss: 0.71177775\n",
      "step: 1260 , loss: 0.8023269\n",
      "test loss: 0.8282175223032634 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1265 , loss: 0.8937738\n",
      "step: 1270 , loss: 0.8236231\n",
      "step: 1275 , loss: 0.84367603\n",
      "step: 1280 , loss: 0.7966063\n",
      "test loss: 0.8229678233464559 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 1285 , loss: 0.80242664\n",
      "-----------epoch: 4\n",
      "step: 1290 , loss: 0.72297865\n",
      "step: 1295 , loss: 0.79245424\n",
      "step: 1300 , loss: 0.78857416\n",
      "test loss: 0.817910615603129 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 1305 , loss: 0.7107264\n",
      "step: 1310 , loss: 0.86706144\n",
      "step: 1315 , loss: 0.7308916\n",
      "step: 1320 , loss: 0.7751002\n",
      "test loss: 0.8127945939699809 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 1325 , loss: 0.8980616\n",
      "step: 1330 , loss: 0.90855265\n",
      "step: 1335 , loss: 0.85461485\n",
      "step: 1340 , loss: 0.7364589\n",
      "test loss: 0.8081120530764262 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 1345 , loss: 0.99985945\n",
      "step: 1350 , loss: 0.76015246\n",
      "step: 1355 , loss: 0.9292837\n",
      "step: 1360 , loss: 0.83486235\n",
      "test loss: 0.8033873875935872 , f1: 1.0\n",
      "New Record!\n",
      "step: 1365 , loss: 0.908734\n",
      "step: 1370 , loss: 0.81599987\n",
      "step: 1375 , loss: 0.7219789\n",
      "step: 1380 , loss: 0.75644445\n",
      "test loss: 0.7988976240158081 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1385 , loss: 1.0153941\n",
      "step: 1390 , loss: 0.8227379\n",
      "step: 1395 , loss: 0.6400237\n",
      "step: 1400 , loss: 0.7182628\n",
      "test loss: 0.7944345156351725 , f1: 0.26262626262626265\n",
      "No Improvement.\n",
      "step: 1405 , loss: 0.7436759\n",
      "step: 1410 , loss: 0.7383703\n",
      "step: 1415 , loss: 0.72312963\n",
      "step: 1420 , loss: 0.7053299\n",
      "test loss: 0.790166954199473 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 1425 , loss: 0.73598266\n",
      "step: 1430 , loss: 0.81832683\n",
      "step: 1435 , loss: 0.6551295\n",
      "step: 1440 , loss: 0.76931\n",
      "test loss: 0.7858943740526835 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 1445 , loss: 0.7826397\n",
      "step: 1450 , loss: 0.6960014\n",
      "step: 1455 , loss: 0.8150833\n",
      "step: 1460 , loss: 0.88744026\n",
      "test loss: 0.781820281346639 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 1465 , loss: 0.7969446\n",
      "step: 1470 , loss: 0.61295354\n",
      "step: 1475 , loss: 0.64658076\n",
      "step: 1480 , loss: 0.7398354\n",
      "test loss: 0.7776707768440246 , f1: 0.4444444444444445\n",
      "No Improvement.\n",
      "step: 1485 , loss: 0.7349453\n",
      "step: 1490 , loss: 0.86357707\n",
      "step: 1495 , loss: 0.6908379\n",
      "step: 1500 , loss: 0.8299153\n",
      "test loss: 0.7735886335372925 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 1505 , loss: 0.7858705\n",
      "step: 1510 , loss: 0.6511382\n",
      "step: 1515 , loss: 0.799727\n",
      "step: 1520 , loss: 0.7219115\n",
      "test loss: 0.769901450475057 , f1: 0.2745098039215686\n",
      "No Improvement.\n",
      "step: 1525 , loss: 0.74172217\n",
      "step: 1530 , loss: 0.8293713\n",
      "step: 1535 , loss: 0.6859836\n",
      "step: 1540 , loss: 0.680329\n",
      "test loss: 0.7662149866422018 , f1: 0.42857142857142855\n",
      "No Improvement.\n",
      "step: 1545 , loss: 0.86851525\n",
      "step: 1550 , loss: 0.8535457\n",
      "step: 1555 , loss: 0.7440751\n",
      "step: 1560 , loss: 0.7742651\n",
      "test loss: 0.7625665505727132 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1565 , loss: 0.6450501\n",
      "step: 1570 , loss: 0.7702032\n",
      "step: 1575 , loss: 0.8762203\n",
      "step: 1580 , loss: 0.8617678\n",
      "test loss: 0.7590837160746257 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1585 , loss: 0.93520945\n",
      "step: 1590 , loss: 0.78242064\n",
      "step: 1595 , loss: 0.6692935\n",
      "step: 1600 , loss: 0.78866184\n",
      "test loss: 0.755617101987203 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1605 , loss: 0.6638709\n",
      "-----------epoch: 5\n",
      "step: 1610 , loss: 0.8755305\n",
      "step: 1615 , loss: 0.7086563\n",
      "step: 1620 , loss: 0.6504538\n",
      "test loss: 0.7521177927652994 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1625 , loss: 0.7118533\n",
      "step: 1630 , loss: 0.7156203\n",
      "step: 1635 , loss: 0.8191619\n",
      "step: 1640 , loss: 0.87940663\n",
      "test loss: 0.748904538154602 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 1645 , loss: 0.7184312\n",
      "step: 1650 , loss: 0.7800933\n",
      "step: 1655 , loss: 0.92830044\n",
      "step: 1660 , loss: 0.6445402\n",
      "test loss: 0.7457854191462199 , f1: 0.45945945945945943\n",
      "No Improvement.\n",
      "step: 1665 , loss: 0.78915143\n",
      "step: 1670 , loss: 0.70929027\n",
      "step: 1675 , loss: 0.69015\n",
      "step: 1680 , loss: 0.8703433\n",
      "test loss: 0.7424144744873047 , f1: 0.29629629629629634\n",
      "No Improvement.\n",
      "step: 1685 , loss: 0.66338074\n",
      "step: 1690 , loss: 0.8385724\n",
      "step: 1695 , loss: 0.7569391\n",
      "step: 1700 , loss: 0.7002325\n",
      "test loss: 0.739298943678538 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 1705 , loss: 0.73342526\n",
      "step: 1710 , loss: 0.7286017\n",
      "step: 1715 , loss: 0.87439126\n",
      "step: 1720 , loss: 0.8015227\n",
      "test loss: 0.7362746318181356 , f1: 0.22972972972972971\n",
      "No Improvement.\n",
      "step: 1725 , loss: 0.6187208\n",
      "step: 1730 , loss: 0.6792098\n",
      "step: 1735 , loss: 0.7120116\n",
      "step: 1740 , loss: 0.57180494\n",
      "test loss: 0.7332362771034241 , f1: 0.31578947368421056\n",
      "No Improvement.\n",
      "step: 1745 , loss: 0.63296664\n",
      "step: 1750 , loss: 0.7680102\n",
      "step: 1755 , loss: 0.7227026\n",
      "step: 1760 , loss: 0.6487233\n",
      "test loss: 0.7301568706830343 , f1: 0.4736842105263158\n",
      "No Improvement.\n",
      "step: 1765 , loss: 0.52766603\n",
      "step: 1770 , loss: 0.78907984\n",
      "step: 1775 , loss: 0.7635127\n",
      "step: 1780 , loss: 0.83987427\n",
      "test loss: 0.7274307688077291 , f1: 0.3063063063063063\n",
      "No Improvement.\n",
      "step: 1785 , loss: 0.6983449\n",
      "step: 1790 , loss: 0.93023777\n",
      "step: 1795 , loss: 0.6490507\n",
      "step: 1800 , loss: 0.7153687\n",
      "test loss: 0.7246424198150635 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 1805 , loss: 0.6799706\n",
      "step: 1810 , loss: 0.60335815\n",
      "step: 1815 , loss: 0.6767705\n",
      "step: 1820 , loss: 0.6729884\n",
      "test loss: 0.7219260334968567 , f1: 0.2745098039215686\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1825 , loss: 0.8707172\n",
      "step: 1830 , loss: 0.70080113\n",
      "step: 1835 , loss: 0.50136185\n",
      "step: 1840 , loss: 0.65137464\n",
      "test loss: 0.7191496372222901 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 1845 , loss: 0.5800684\n",
      "step: 1850 , loss: 0.6437214\n",
      "step: 1855 , loss: 0.70143056\n",
      "step: 1860 , loss: 0.7048693\n",
      "test loss: 0.7164453486601512 , f1: 1.0\n",
      "New Record!\n",
      "step: 1865 , loss: 0.79211366\n",
      "step: 1870 , loss: 0.61585444\n",
      "step: 1875 , loss: 0.7532644\n",
      "step: 1880 , loss: 0.72311187\n",
      "test loss: 0.7139386455217998 , f1: 0.22222222222222224\n",
      "No Improvement.\n",
      "step: 1885 , loss: 0.56182885\n",
      "step: 1890 , loss: 0.5821235\n",
      "step: 1895 , loss: 0.61996716\n",
      "step: 1900 , loss: 0.9036221\n",
      "test loss: 0.711421529452006 , f1: 0.2857142857142857\n",
      "No Improvement.\n",
      "step: 1905 , loss: 0.521705\n",
      "step: 1910 , loss: 0.5515935\n",
      "step: 1915 , loss: 0.6188651\n",
      "step: 1920 , loss: 0.6932901\n",
      "test loss: 0.7090383172035217 , f1: 0.48717948717948717\n",
      "No Improvement.\n",
      "step: 1925 , loss: 0.5619769\n"
     ]
    }
   ],
   "source": [
    "checkpoint = './jiage.ckpt'\n",
    "work_dir = 'jiage_graph'\n",
    "test_size = 1000\n",
    "test_loss = 10000\n",
    "\n",
    "stop_early = 0\n",
    "stop = 10\n",
    "best_f1 = 0\n",
    "\n",
    "print_step = 5\n",
    "update_step = 20\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    cnn = TextCNN(seq_len=80, embedding=embedding_matrix, filter_sizes=[2, 3, 4], num_filters=256, num_classes=4, l2_reg=0.01)\n",
    "    cnn.create_placeholder()\n",
    "    cnn.create_variable()\n",
    "    cnn.create_model()\n",
    "    cnn.create_loss(lr=0.00001)\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        writer = tf.summary.FileWriter(work_dir, sess.graph)\n",
    "        \n",
    "        # global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        for batch_x, batch_y in get_batch(train_X, train_Y, num_epochs=6, batch_size=30):\n",
    "            feed_dict = {\n",
    "                cnn.x: batch_x,\n",
    "                cnn.y: batch_y,\n",
    "                cnn.keep_prob: 0.6,\n",
    "            }\n",
    "            # loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            # loss_smry, loss, _ = sess.run([loss_summary, cnn.loss, cnn.train_op], feed_dict=feed_dict)\n",
    "            loss, _ = sess.run([cnn.loss, cnn.train_op], feed_dict=feed_dict)\n",
    "            step += 1\n",
    "            \n",
    "            # writer.add_summary(loss_smry, step)\n",
    "           \n",
    "            \n",
    "            if step % print_step == 0:\n",
    "                print('step:', step, ', loss:', loss)\n",
    "            if step % update_step == 0:\n",
    "                val_loss = []\n",
    "                for batch_x, batch_y in get_batch(val_X, val_Y, num_epochs=1, batch_size=20, echo=False):\n",
    "                    feed_dict = {\n",
    "                        cnn.x: batch_x,\n",
    "                        cnn.y: batch_y,\n",
    "                        cnn.keep_prob: 1,\n",
    "                    }\n",
    "                    y_pred, loss, _ = sess.run([cnn.predictions, cnn.loss, cnn.train_op], feed_dict=feed_dict)\n",
    "                    val_loss.append(loss)\n",
    "                update_loss = sum(val_loss)/len(val_loss)\n",
    "                f1 = get_f1(batch_y, y_pred)\n",
    "                print('test loss:', update_loss, ', f1:', f1)\n",
    "                if (update_loss <= test_loss) and (f1 > best_f1):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "                    test_loss = update_loss\n",
    "                    if f1 < 0.9:\n",
    "                        best_f1 = f1\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                if best_f1 > 0.5:\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                else:\n",
    "                    stop_early = 0\n",
    "                   \n",
    "                update_loss = 0\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2364, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/test_public.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "test_X, word_count, unk_count = convert_to_ints(test['content'], vocab2int, word_count, unk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./jiage.ckpt\n",
      "(2364,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_graph = tf.Graph()\n",
    "final_result = None\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    output = loaded_graph.get_tensor_by_name('output/predictions:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for t_x in get_batch(test_X, num_epochs=1, batch_size=591, echo=False, shuffle=False):\n",
    "        result = sess.run(output, {x: t_x, keep_prob: 1.0})\n",
    "        if final_result is None:\n",
    "            final_result = result\n",
    "        else:\n",
    "            final_result = np.concatenate((final_result, result), axis=0)\n",
    "print(final_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XuPwKCnA2fqNh5vm</td>\n",
       "      <td>欧蓝德，价格便宜，森林人太贵啦！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2jNbDn85goX3IuPE</td>\n",
       "      <td>楼主什么时候提的车，南昌优惠多少啊</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hLgEADQ8sUnvGFK9</td>\n",
       "      <td>吉林，2.5优惠20000，送三年九次保养，贴膜</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nZmM7LQsfr03wUaz</td>\n",
       "      <td>便宜2万的豪华特装，实用配制提升，优惠还给力，确实划算。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pwd8MnrthDqLZafe</td>\n",
       "      <td>如果实在想买就等车展期间，优惠2万，我24.98万入的2.5豪</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cudSKGTyMLbIt8k3</td>\n",
       "      <td>2.0时尚优惠两万现金吗？还有其他赠品吗？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U5RMQtopwFzDVxHm</td>\n",
       "      <td>27.5 相比较优惠的少</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kh0ayJjuTrzwPAs1</td>\n",
       "      <td>综合优惠两万，不是现金优惠两万，送的垃圾东西都包含在内.大忽悠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YZovRtEAyQJgTi1I</td>\n",
       "      <td>差不多15000左右的优惠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BeSIAUzoTWH9muYl</td>\n",
       "      <td>恭喜恭喜，这个配置性价比很高</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SXYTBWg6FxuboMnd</td>\n",
       "      <td>谁还用车载导航呀！用手机导航吧及时性好！而且到港后装的机头不行，在意的话建议去换一个机头顺便...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RDgTmzW9xBfY6pGv</td>\n",
       "      <td>你好，我也是受够了16款的垃圾导航。这个美版主机哪里可以买到？安装后可以完美使用吗，有什么问...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8cMwzsxkL4XtuDG1</td>\n",
       "      <td>森林人导航确实垃圾。再好的车再好的导航都没有手机导航好用，我提车后只用了二个多月的车载导航就...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MOUrvjDsIJpbw6dQ</td>\n",
       "      <td>导航现在都用手机的，音箱没有说的那么差，就是低音差点，人声其实还可以。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t01PVoDSd9Lsy5HN</td>\n",
       "      <td>黑屏经常发生啊和手机蓝牙，连接后电话打进来根本听不到对方在说什么，想指往导航功能就会让你上火</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>icMQmPJA9Sv0KGTw</td>\n",
       "      <td>传感器是在轮毂里面的，连着气门嘴的，跟轮胎没关系</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VQkjM1657sIroCf3</td>\n",
       "      <td>60肯定是不行的。 原车轮胎的直径=215*0.65*2 15*25.4（一英寸=25.4毫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1rGSYnOx9Fb2ULQX</td>\n",
       "      <td>那你们换的什么中控呀？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SgNFp8ixQX1HA64y</td>\n",
       "      <td>都这样，提高油号有所减轻。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sO6Ac1mThYBKyl4M</td>\n",
       "      <td>你的真费油，我在北京开才9.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          content_id                                            content  pred\n",
       "0   XuPwKCnA2fqNh5vm                               欧蓝德，价格便宜，森林人太贵啦！         0\n",
       "1   2jNbDn85goX3IuPE                                  楼主什么时候提的车，南昌优惠多少啊     0\n",
       "2   hLgEADQ8sUnvGFK9                           吉林，2.5优惠20000，送三年九次保养，贴膜     0\n",
       "3   nZmM7LQsfr03wUaz                       便宜2万的豪华特装，实用配制提升，优惠还给力，确实划算。     0\n",
       "4   pwd8MnrthDqLZafe                    如果实在想买就等车展期间，优惠2万，我24.98万入的2.5豪     0\n",
       "5   cudSKGTyMLbIt8k3                          2.0时尚优惠两万现金吗？还有其他赠品吗？         0\n",
       "6   U5RMQtopwFzDVxHm                                       27.5 相比较优惠的少     0\n",
       "7   Kh0ayJjuTrzwPAs1               综合优惠两万，不是现金优惠两万，送的垃圾东西都包含在内.大忽悠.         0\n",
       "8   YZovRtEAyQJgTi1I                                  差不多15000左右的优惠         0\n",
       "9   BeSIAUzoTWH9muYl                                     恭喜恭喜，这个配置性价比很高     0\n",
       "10  SXYTBWg6FxuboMnd  谁还用车载导航呀！用手机导航吧及时性好！而且到港后装的机头不行，在意的话建议去换一个机头顺便...     0\n",
       "11  RDgTmzW9xBfY6pGv  你好，我也是受够了16款的垃圾导航。这个美版主机哪里可以买到？安装后可以完美使用吗，有什么问...     0\n",
       "12  8cMwzsxkL4XtuDG1  森林人导航确实垃圾。再好的车再好的导航都没有手机导航好用，我提车后只用了二个多月的车载导航就...     0\n",
       "13  MOUrvjDsIJpbw6dQ                导航现在都用手机的，音箱没有说的那么差，就是低音差点，人声其实还可以。     0\n",
       "14  t01PVoDSd9Lsy5HN     黑屏经常发生啊和手机蓝牙，连接后电话打进来根本听不到对方在说什么，想指往导航功能就会让你上火     0\n",
       "15  icMQmPJA9Sv0KGTw                           传感器是在轮毂里面的，连着气门嘴的，跟轮胎没关系     0\n",
       "16  VQkjM1657sIroCf3  60肯定是不行的。 原车轮胎的直径=215*0.65*2 15*25.4（一英寸=25.4毫...     0\n",
       "17  1rGSYnOx9Fb2ULQX                                    那你们换的什么中控呀？         0\n",
       "18  SgNFp8ixQX1HA64y                                      都这样，提高油号有所减轻。     0\n",
       "19  sO6Ac1mThYBKyl4M                                    你的真费油，我在北京开才9.2     0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pred'] = final_result\n",
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()\n",
    "test.to_csv('../output/jiage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
